\chapter{Evaluation}
%TODO something about the moving taget (mainly how does injecting change look like, and how does it scale in this process?
This section evaluates and discusses the implementations from the thesis so far, establishing some quantifiable goals while discussing the general feasibility of the approach. It both raises some issues that needs to be solved before in order to improve the method, and explains some of the side-benefits that followed the approach.

\section{Challenges}
As already mentioned in chapter \ref{ch:design}, the open issue on path coverage -- and especially scalability of it, remains.

\section{General usefulness}
%TODO VERY IMPORTANT, as this gives the motivation for filling the thesis with crap about OpenReception project and architecture.
The process described in this thesis, fit quite nicely into the OpenReception project. Is it a coincidence that the architecture described in chapter \ref{ch:background} fit very nicely. Another thing, is that the use cases -- in general -- are quite simple. This can also be seen in appendix \ref{appendix:use-cases}. The scope of the OpenReception project has not been to provide a complex usage model, but rather the opposite. The target businesses for the product, focus on low average call-handling time and, thus, will try to keep the use cases simple -- in that effort.\\\\
Other development projects, that have more complex use cases, may experience...


\section{Parallelism}
\label{sec:parallelism}
A section on multiple actors and race conditions. How do you create a use case that contains multiple simultaneous instances of actors that perform the same action synchronously? Basically asserting parallel properties.\todo{.\\turn this into a real section}\\\\
We have a use case, where two Receptionist actor battles for the same call. In order to test this, we may wish to describe the scenario like so -- assuming a call is has arrived and is ready for pickup:
\begin{enumerate}
 \item Receptionist 1 tries to pickup call
 \item Receptionist 2 tries to pickup call
 \item Call is assigned to either Receptionist
\end{enumerate}
%TODO add sequence diagram.
A postcondition may read ``The call is assigned to \emph{only} one receptionist'', in order to emphasize on the actual intended behavior of the system, but the description above should be sufficient.\\
Studying the use case a bit closer, what is actually implied is that 1. happens simultaneously with 2. In practice, it would mean that a test would have to emulate the simultaneous behavior by spawning multiple threads and collecting their return values once they have terminated. But there is another problem with the scenario above, which is that the test tools do not know when to parallelize. As of now, every entry in a use case scenario is modeled as a synchronous action and will wait until the entry has completed its execution before starting the next one.\\\\
A method for solving this, is to add the asynchronism in the mapped test code, but this is a very bad idea. This would lead to very unexpected behavior if requirements change in the specific block. This would lead to treads being spawned, expecting to perform a specific action that no longer existed in the requirements, perhaps deadlocking while waiting for an event to happen -- or change the state of the system that would lead an error later in the test.\\
A better way of solving it, is to add a keyword. For instance \textbf{simultaneously}. So the use case would then read;
\begin{enumerate}
 \item Receptionist 1 tries to pickup call
 \item Receptionist 2 tries to pickup call \textbf{simultaneously}
 \item Call is assigned to either Receptionist
\end{enumerate}
Making the keyword refer in 2. refer to the previous entry, 1. This feature is neither implemented, nor conceptualized further, but included in the discussion as it is a actual problem that was encountered during the development of the 2nd iteration of the tests. There has been developed an \emph{ad-hoc} test that uses the spawn-threads-and-collect method introduced above so there exists a technical solution for the problem.
%\section{Applicablity to different methodologies} See http://en.wikipedia.org/wiki/Requirements_analysis
% Waterfall, Prototype model, Incremental, Iterative, V-Model, Spiral, Scrum, Cleanroom, RAD ...


\subsection{Side benefits}
%One of the major benefits is that it makes other black-box tests easier, as we can re-use components from the framework.
% Treating the system global state enforces a notion within the programmer how the specific activity acutally mutates the global state of the system.

Writing integration tests is definitely a lot easier. Writing up tests that asserts that an edge case is not reached. The code becomes very verbose in the way that you state the test from the point of view of an actor performing an action. Informally, what is actually possible, is writing a requirement as a test. Not as in the topic of this thesis, but as a hand-coded test that asserts that a feature is present. Having the tools (the test framework) makes it very intuitive and easy to write. The concrete code for testing the example outlined in section \ref{sec:parallelism} is shown in listing \ref{lst:test-code-single-call-allocation}. This code is not generated.
\begin{lstlisting}[style=Dart, caption=Test code for single call allocation,label={lst:test-code-single-call-allocation}]
  static void pickupAllocatedCall(Receptionist receptionist, 
                                  Receptionist receptionist2, 
                                  Customer callee) {
    String receptionNumber = '12340002';
    Model.Call allocatedCall;
    
    log.info ('Customer ${callee.name} dials ${receptionNumber}');
    callee.dial (receptionNumber);
    log.info ('Receptionist ${receptionist.user.name} hunts call.');
    allocatedCall = receptionist.huntNextCall();
   
    expect (receptionist2.pickup(call), throwsA(Forbidden));
    log.info('Test done');
  }
\end{lstlisting}
Some may argue that this piece of code maps very nice to a requirement. While it may not be entirely possible to completely generated it from one, a requirement can most definitely be derived from the code. The example also captures an error condition, which may be challenging to describe from a use case in sufficient detail so that test generation is possible.\\\\
Another side benefit from having use cases written as tests -- whether they are generated or not -- gives new developers a good reference point on how the system is intended to work. By reading the class files representing actor and concepts, and how they interface with the system provides a good high-level overview of the general architecture, and how it should be used.

\subsection{Differences between first and second iteration}
In the first implementation of the test tools (see section \ref{sec:1st-iteration-test-specification}), there was no shared knowledge of the domain. The tests were written in a different programming language than the main codebase, and interfacing was manual -- even though the test case generation were automated.
The second iteration also treated tests as an intricate part of the main project, rather than an auxiliary part.
\begin{figure}[!hbpt]
\centering
\includegraphics[width=1.0\textwidth]{\imgdir jenkins-build-trend-interation-1}
\caption{Iteration 1 Jenkins build trend. Blue area is total tests, and red area is failures}
\label{fig:jenkins-build-trend-interation-1}
\end{figure}


\begin{figure}[!hbpt]
\centering
\includegraphics[width=1.0\textwidth]{\imgdir jenkins-build-trend-interation-2}
\caption{Iteration 2 Jenkins build trend. Blue area is total tests, and red area is failures}
\label{fig:jenkins-build-trend-interation-2}
\end{figure}

%TODO what happens when you need an extra resource?
\section{Feasibility}
Iteration 1 build trend in figure \ref{fig:jenkins-build-trend-interation-1}\footnote{Image quality is low, but as the Jenkins server that ran the test has been \emph{physically} garbage collected, it was the best that could be procured.}....

To be able find a quantitative measure for how much extra work this method is, the number of code lines were enumerated, and support-only files were counted as overhead. Table \ref{tab:loc-metrics}
\begin{table}[!htbp]
\begin{tabular}{ | l | r | r | r |}
   \hline
   Project        & Total (kLoc) & Test support (kLoc) & Overhead \\ \hline
   Test framework & 8.9          & 1.1                 & 10\%     \\
   Phonio         & 2            & 2                   & 100\%    \\
   \hline
   \hline
   Total          & 10.9         & 3.1                 & 28\%     \\
   \hline
\end{tabular}
\caption{Test framework metrics}
\label{tab:loc-metrics}
\end{table}
The rest of the test framework code are the (manually written) tests
As lines of code may be a bad measure\cite{fraser2013does}, the relative number of commits to the revision control system (RCS) was counted. 

Looking at the git commits, may result in more accurate numbers.

\begin{table}[!htbp]
\begin{tabular}{ | l | r | r | r |}
   \hline
   Project        & Total (commits) & Test support (commits) & Overhead \\ \hline
   Test framework & 355             & 53                     & 14.9\%     \\
   \hline
\end{tabular}
\caption{Number of commits (snapshot 2\textsuperscript{nd} of July)}
\label{tab:metrics-commit-count}
\end{table}
Taking into account the total relatively low number of commits, and the young age of the repository (approximately 5 months), there is peculiar correlation between our overhead, and the overhead from a larger study of test-driven development methods\cite{george2003}. In this study, they approximate that roughly 16\% more time is spent on development when using test-driven development methods. This matches our initial findings quite well, and indicate direct cost increase in software development.
   
%How much effort is there involved in changing requirements, or implementation.
%Discuss code change on different levels; interface (component), function or data model.
%Discuss which impact a use case step change has, and most importantly; does it


%is this always possible? Are we lucky to have found a good fit for this procedure? Which cases could be thought, not to support this method?
%TODO note;
\subsection{Relationship to TDD}
In practice; the approach presented in this thesis is a variant of TDD, but injects some elements from MDE and increases the test coverage by automatically generating the tests from use case branches.

%Important bit.

%another note; it was a good experience for me, as a software engineer to be able communicate how I wanted the system to interact internally via tests. It is easier for code-oriented people to relate to code, rather than diagrams and designs.

\subsection{Is if feasible}
Basically; no. Can it be used? To some extent. The process is nice, but the mapping simply unfolds into a too large state space with a stucture so complex that is impossible for an end-user to specify it, and very hard for a trained professional to elaborate it. It may be feasible once model-based software engineering reaches a level where behavioral model can be applied to domain models, and thus ...
%As of now, an ad-hoc templating system is in place. A real templating system should be used in a real product.
The test coverage generation may be of some use, and also requirement changing
But, alas, it is too early to tell. More research is needed.


\subsection{Recommended test framework guidelines}
%Use allocation pools
%Use interfaces a objects.

\section{Caveats}
The case study system and test system has been developed by the same group of people, which have also played a large role in the use cases creation and refinement process. This is problematic when the general applicability of the approach presented in this thesis needs to be evaluated. Having the same people work on requirements, implementation and tests lead to a unified understanding of the system as a whole, and a very short path from requirement change, to implementation change. While this is generally a good thing, it does not scale to larger projects, and important domain-specific details may unknowingly go undocumented, as everyone has a common understanding of how things work overall. The problem with this, is also that requirements may have an ``artificial feel'', in the way that they reflect the actual workings of the system under development, rather than the intended workings.\\\\
For this evaluation, it means that our requirements may be better suited for acceptance-test-writing than others, trying the same approach. It can however be argued, that as any (good) requirement should be testable\cite{hull2010requirements}, then it shouldn't matter significantly how the project groups are structured.

%TODO It motivates the question; what does this use case step mean in the system - from an interface point of view.
\section{Summary}
The TDD study also showed that tdd approaches do produce higher quality software, so the extra time spend \emph{may} be worthwhile.

%Test case; when does it end? in our case, the message-sending archtecture is defined to be a work-queue where the dispatcher is decoupled from the message sending, which is merely an enqueuer. If the postcondition for our test case had been; "Message is received by contact", then the test-macro function becomes increasingly large.
%Test cases may further introduce dependencies, such as messageStore
The number of paths generated quickly makes this procedure fail. Additional heuristics, such as strict design guidelines for use case writing -- espcially in regards to decomposition and relation to each other -- needs to be applied before it can be applied to larger systems.