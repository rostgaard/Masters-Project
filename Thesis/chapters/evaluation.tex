\chapter{Evaluation}
%TODO something about the moving taget (mainly how does injecting change look like, and how does it scale in this process?
This section evaluates and discusses the implementation from the thesis, establishing some quantifiable goals while discussing the general feasibility of the approach. It both raises some issues that needs to be solved before in order to improve the method, and identifies some of the side-benefits that followed the approach.


\section{General usefulness}
%TODO VERY IMPORTANT, as this gives the motivation for filling the thesis with crap about OpenReception project and architecture.
%is this always possible? Are we lucky to have found a good fit for this procedure? Which cases could be thought, not to support this method?

The process described in this thesis, fit quite nicely into the case study system. Is it a coincidence that the architecture described in chapter \ref{ch:background} fit very nicely. Another thing, is that the use cases -- in general -- are quite simple. This can also be seen in appendix \ref{appendix:use-cases}. The scope of the case study project has not been to provide a complex usage model, but rather the opposite. The target businesses for the product, focus on low average call-handling time and, thus, will try to keep the use cases simple -- in that effort. Another thing, that may have an effect on the specific fit, is the fact that the case study system uses a stateless architecture. This may make the use case steps easier to write, as the state is carried by the tests, rather than the services.\medskip

\noindent Other development projects, that have more complex use cases or interfaces, may experience larger complexity in writing mapping functions that realize their use case.

\section{Impact of changes}
This section tries to uncover the relative additional workload of this approach, from a requirement or implementation change perspective. Changes, thus, discussed from these two points of view.

\subsection{Requirement changes}
One of the most relevant question of this project is; how does requirement changes impact the generated tests? The answer to this, is defined by the characteristic of the change, which is either the modification of an existing requirement, or the addition of one.\medskip

\noindent The action of modifying an existing requirement, needs to follow a very specific procedure. First the use case needs to modified to accommodate the change, which could involve the rephrasing a single use case step, the addition or removal of an extension, or the (re)definition of a concept or actor. When this is done, the template will need to be changed to match the new/changed steps by either modifying existing mapping functions, or writing up new ones. The regeneration of tests is possible to do automatically by creating an association between use case and template, which is not present in the current implementation. This step is, thus manual. In essence; the workload of changing requirements is relative 1:1 relationship between changed/added steps and mapping functions. So one step changed, means one mapping function needs to be changed.\medskip

\noindent An addition of a new requirement, is defined by the same 1:1 workload, but has the additional task of creating the template needed to map to the implementation. If the new requirement defines a new actor -- or domain concept -- additional work must be calculated to build up the classes for these. A test tool overhead is presented in table \ref{tab:metrics-commit-count} that suggest that at least 15\% extra time is needed for writing up these classes.\medskip

\noindent The removal of a requirements or used step mappings may be detected by a good static analysis of the generated source code. It will inform the developer of unused functions, that are suitable for removal.

\subsection{Implementation change}
Most implementation changes should be transparent to the tests, and only interfaces changes would really have an impact. As the test support tools from the case study system share the interfaces classes with the implementation, inconsistencies will be detected in a re-compilation of the test case sources. It is therefor important that the test support tools track implementation. In the case study system, this is done by putting an incremented version number on every release of the shared framework (see section \ref{ssec:openreception-framework}). The test runner will upgrade to the latest framework version, prior to building and running, hereby ensuring that the tests work with the latest interfaces.\medskip

\noindent If there is no sharing of interface code between tests and the developed system, it will very challenging and time consuming to keep the tests synchronized with the system. This is discussed further in section \ref{ssec:first-and-second-iteration-differences}.\medskip

\noindent Any change to actors or concepts seem unlikely, as they are created solely for the purpose of tests, and should only be changed if the requirements change.

\section{Challenges}
%TODO finalize this section
As already mentioned in chapter \ref{ch:design}, the open issue on path coverage -- and especially scalability of it, remains.

\subsection{Parallelism}
\label{sec:parallelism}
A case that is not able to be covered by this approach, is when a use case contains multiple actors that perform the same action simultaneously.\medskip

\noindent We have a use case, where two Receptionist actor battles for the same call. In order to test this, we may wish to describe the scenario like so -- assuming a call is has arrived and is ready for pickup:
\begin{enumerate}
 \item Receptionist 1 tries to pickup call
 \item Receptionist 2 tries to pickup call
 \item Call is assigned to either Receptionist
\end{enumerate}
%TODO add sequence diagram.
A postcondition may read ``The call is assigned to \emph{only} one receptionist'', in order to emphasize on the actual intended behavior of the system, but the description above should be sufficient.\medskip

\noindent Studying the use case a bit closer, what is actually implied is that 1. happens simultaneously with 2. In practice, it would mean that a test would have to emulate the simultaneous behavior by spawning multiple threads and collecting their return values once they have terminated. But there is another problem with the scenario above, which is that the test tools do not know when to parallelize. As of now, every entry in a use case scenario is modeled as a synchronous action and will wait until the entry has completed its execution before starting the next one.\medskip

\noindent A method for solving this, is to add the asynchronism in the mapped test code, but this is a very bad idea. This would lead to very unexpected behavior if requirements change in the specific block. This would lead to treads being spawned, expecting to perform a specific action that no longer existed in the requirements, perhaps deadlocking while waiting for an event to happen -- or change the state of the system that would lead an error later in the test.\medskip

\noindent A better way of solving it, is to add a keyword. For instance \textbf{simultaneously}. So the use case would then read;
\begin{enumerate}
 \item Receptionist 1 tries to pickup call
 \item Receptionist 2 tries to pickup call \textbf{simultaneously}
 \item Call is assigned to either Receptionist
\end{enumerate}
Making the keyword refer in 2. refer to the previous entry, 1. This feature is neither implemented, nor conceptualized further, but included in the discussion as it is a actual problem that was encountered during the development of the 2nd iteration of the tests. There has been developed an \emph{ad-hoc} test that uses the spawn-threads-and-collect method introduced above so there exists a technical solution for the problem.

\section{Benefits}
\subsection{Side effects}
%One of the major benefits is that it makes other black-box tests easier, as we can re-use components from the framework.
% Treating the system global state enforces a notion within the programmer how the specific activity acutally mutates the global state of the system.

Writing integration tests is definitely a lot easier. Writing up tests that asserts that an edge case is not reached. The code becomes very verbose in the way that you state the test from the point of view of an actor performing an action. Informally, what is actually possible, is writing a requirement as a test. Not as in the topic of this thesis, but as a hand-coded test that asserts that a feature is present. Having the tools (the test framework) makes it very intuitive and easy to write. The concrete code for testing the example outlined in section \ref{sec:parallelism} is shown in listing \ref{lst:test-code-single-call-allocation}. This code is not generated.
\begin{lstlisting}[style=Dart, caption=Test code for single call allocation,label={lst:test-code-single-call-allocation}]
  static void pickupAllocatedCall(Receptionist receptionist, 
                                  Receptionist receptionist2, 
                                  Customer callee) {
    String receptionNumber = '12340002';
    Model.Call allocatedCall;
    
    log.info ('Customer ${callee.name} dials ${receptionNumber}');
    callee.dial (receptionNumber);
    log.info ('Receptionist ${receptionist.user.name} hunts call.');
    allocatedCall = receptionist.huntNextCall();
   
    expect (receptionist2.pickup(call), throwsA(Forbidden));
    log.info('Test done');
  }
\end{lstlisting}
Some may argue that this piece of code maps very nice to a requirement. While it may not be entirely possible to completely generated it from one, a requirement can most definitely be derived from the code. The example also captures an error condition, which may be challenging to describe from a use case in sufficient detail so that test generation is possible.\medskip

\noindent Another side benefit from having use cases written as tests -- whether they are generated or not -- gives new developers a good reference point on how the system is intended to work. By reading the class files representing actor and concepts, and how they interface with the system provides a good high-level overview of the general architecture, and how it should be used.


\section{Metrics}
This section presents the empiric evidence gathered with the purpose of determining the usefulness and additional workload factor of the approach in this thesis.
\subsection{Differences between first and second iteration}
\label{ssec:first-and-second-iteration-differences}
The first data set that is collected, are the graphs from our continuous integration service -- Jenkins -- that present the relative number of failures over time, comparing the two first iterations (chapter \ref{chapter:first-iterations}) of the requirements-as-tests approach. The biggest difference in the two approaches is the fact that one of them shares code with the code base of the developed system, and the other does not. This will, thus, serve as the evaluation point. The two datasets have different number of total tests, which is why the evaluation will be done on the basis of the \emph{relative} number of errors, instead of the absolute.\medskip

\noindent In the first implementation of the test tools (see section \ref{sec:1st-iteration-test-specification}), there was no shared knowledge of the domain. The tests were written in a different programming language than the main code base, and interfacing was manual -- even though the test case generation were automated.\medskip
\begin{figure}[!hbpt]
\centering
\includegraphics[width=1.0\textwidth]{\imgdir jenkins-build-trend-interation-1}
\caption{Iteration 1 Jenkins build trend. Blue area is total tests, and red area is failures}
\label{fig:jenkins-build-trend-interation-1}
\end{figure}

\noindent Iteration 1 build trend in figure \ref{fig:jenkins-build-trend-interation-1}\footnote{Image quality is low, but as the Jenkins server that ran the test has been \emph{physically} garbage collected, it was the best that could be procured.} show an increasing relative number of errors compared to the second iteration shown in figure \ref{fig:jenkins-build-trend-interation-2}.\medskip

\noindent The second iteration treated tests as an intricate part of the main project, rather than an auxiliary part, and made use if the framework that was built for the application. This, ultimately led to a much lower failure rate, and the correction of errors much faster. First iteration has many longer-running errors, where second iteration typically fixes the error before new errors arise.
%TODO add conclusive stuff.

\begin{figure}[!hbpt]
\centering
\includegraphics[width=1.0\textwidth]{\imgdir jenkins-build-trend-interation-2}
\caption{Iteration 2 Jenkins build trend. Blue area is total tests, and red area is failures}
\label{fig:jenkins-build-trend-interation-2}
\end{figure}

\subsection{Workload overhead}
To be able find a quantitative measure for how much extra work this method is, the number of code lines were enumerated, and support-only files were counted as overhead. Table \ref{tab:loc-metrics}
\begin{table}[!htbp]
\begin{tabular}{ | l | r | r | r |}
   \hline
   Project        & Total (kLoc) & Test support (kLoc) & Overhead \\ \hline
   Test framework & 8.9          & 1.1                 & 10\%     \\
   Phonio         & 2            & 2                   & 100\%    \\
   \hline
   \hline
   Total          & 10.9         & 3.1                 & 28\%     \\
   \hline
\end{tabular}
\caption{Test framework metrics}
\label{tab:loc-metrics}
\end{table}
The rest of the test framework code are the (manually written) tests
As lines of code may be a bad measure\cite{fraser2013does}, the relative number of commits to the revision control system (RCS) was counted. 

Looking at the git commits, may result in more accurate numbers.

\begin{table}[!htbp]
\begin{tabular}{ | l | r | r | r |}
   \hline
   Project        & Total (commits) & Test support (commits) & Overhead \\ \hline
   Test framework & 355             & 53                     & 14.9\%     \\
   \hline
\end{tabular}
\caption{Number of commits (snapshot 2\textsuperscript{nd} of July)}
\label{tab:metrics-commit-count}
\end{table}
Taking into account the total relatively low number of commits, and the young age of the repository (approximately 5 months), there is peculiar correlation between our overhead, and the overhead from a larger study of test-driven development methods\cite{george2003}. In this study, they approximate that roughly 16\% more time is spent on development when using test-driven development methods. This matches our initial findings quite well, and indicate which the factor of cost increase in software development associated with this approach.

\section{Caveats}
The case study system and test system has been developed by the same group of people, which have also played a large role in the use cases creation and refinement process. This is problematic when the general applicability of the approach presented in this thesis needs to be evaluated. Having the same people work on requirements, implementation and tests lead to a unified understanding of the system as a whole, and a very short path from requirement change, to implementation change. While this is generally a good thing, it does not scale to larger projects, and important domain-specific details may unknowingly go undocumented, as everyone has a common understanding of how things work overall. The problem with this, is also that requirements may have an ``artificial feel'', in the way that they reflect the actual workings of the system under development, rather than the intended workings.\medskip

\noindent For this evaluation, it means that our requirements may be better suited for acceptance-test-writing than others, trying the same approach. It can however be argued, that as any (good) requirement should be testable\cite{hull2010requirements}, then it shouldn't matter significantly how the project groups are structured.

\section{Summary}
The TDD study also showed that tdd approaches do produce higher quality software, so the extra time spend \emph{may} be worthwhile.
%TODO the stateless architecture may also have helped.
%Test case; when does it end? in our case, the message-sending archtecture is defined to be a work-queue where the dispatcher is decoupled from the message sending, which is merely an enqueuer. If the postcondition for our test case had been; "Message is received by contact", then the test-macro function becomes increasingly large.
%Test cases may further introduce dependencies, such as messageStore
The number of paths generated quickly makes this procedure fail. Additional heuristics, such as strict design guidelines for use case writing -- espcially in regards to decomposition and relation to each other -- needs to be applied before it can be applied to larger systems.

\subsection{Relationship to TDD}
In practice; the approach presented in this thesis is a variant of TDD, but injects some elements from MDE and increases the test coverage by automatically generating the tests from use case branches.

%Important bit.

%another note; it was a good experience for me, as a software engineer to be able communicate how I wanted the system to interact internally via tests. It is easier for code-oriented people to relate to code, rather than diagrams and designs.

\subsection{Is if feasible}
Basically; no. Can it be used? To some extent. The process is nice, but the mapping simply unfolds into a too large state space with a stucture so complex that is impossible for an end-user to specify it, and very hard for a trained professional to elaborate it. It may be feasible once model-based software engineering reaches a level where behavioral model can be applied to domain models, and thus ...
%As of now, an ad-hoc templating system is in place. A real templating system should be used in a real product.
The test coverage generation may be of some use, and also requirement changing
But, alas, it is too early to tell. More research is needed.


\subsection{Recommended test framework guidelines}
%Use allocation pools
%Use interfaces a objects.


%TODO The approach may also be used to support a top-down use case driven development approach, that is conceptually similar to test-driven development, in the way that it focuses -- after basic use cases are written -- on the basic functionality of the system from the point of view of a user. Writing the mappings to an implementation, in parallel with writing the implementation 
%TODO It motivates the question; what does this use case step mean in the system - from an interface point of view.