\chapter{Discussion}
%TODO something about the moving taget (mainly how does injecting change look like, and how does it scale in this process?
%TODO is this integrateable in an exisiting development process?
%TODO Check that there is something about long-lived requirements, or kill the section in the introduction.

This chapter discusses and evaluates the implementation from the thesis, establishing some quantifiable goals, while discussing the general usefulness and feasibility of the approach. It also raises some issues that need to be solved before in order to improve the technique of the thesis. Additionally it identifies a number of the side-benefits that followed the approach and evaluates the overhead associated with it, based on collected data. The General applicability is discussed in the next section. This is followed by a section that describes the impact of changes in either requirements or system under test, how this technique handles it, and how much extra work is associated with it.


\section{General applicability}
The process described in this thesis fit quite nicely into the case study system. Is it a coincidence that the architecture described in chapter \ref{ch:background} fit very nicely. Another thing, is that the use cases -- in general -- are quite simple. This can also be seen in appendix \ref{appendix:use-cases}. The scope of the case study project has not been to provide a complex usage model, but rather the opposite. The target businesses for the product, focus on low average call-handling time and, thus, will try to keep the use cases simple -- in that effort. Another thing, that may have an effect on the specific fit, is the fact that the case study system uses a stateless architecture. This may make the use case steps easier to write, as the state is carried by the tests, rather than the services.\medskip

\noindent Other development projects, that have more complex use cases or interfaces, may experience larger complexity in writing mapping functions that realize their use case.

\section{Impact of changes}
This section tries to uncover the relative additional workload of this approach, from a requirement or implementation change perspective. Changes, thus, discussed from these two points of view.

\subsection{Requirement changes}
One of the most relevant question of this project is; how does requirement changes impact the generated tests? The answer to this, is defined by the characteristic of the change, which is either the modification of an existing requirement, or the addition of one.\medskip

\noindent The action of modifying an existing requirement, needs to follow a very specific procedure. First the use case needs to modified to accommodate the change, which could involve the rephrasing a single use case step, the addition or removal of an extension, or the (re)definition of a concept or actor. When this is done, the template will need to be changed to match the new/changed steps by either modifying existing mapping functions, or writing up new ones. The regeneration of tests is possible to do automatically by creating an association between use case and template, which is not present in the current implementation. This step is, thus manual. In essence; the workload of changing requirements is relative 1:1 relationship between changed/added steps and mapping functions. So one step changed, means one mapping function needs to be changed.\medskip

\noindent An addition of a new requirement, is defined by the same 1:1 workload, but has the additional task of creating the template needed to map to the implementation. If the new requirement defines a new actor -- or domain concept -- additional work must be calculated to build up the classes for these. A test tool overhead is presented in table \ref{tab:metrics-commit-count} that suggest that at least 15\% extra time is needed for writing up these classes.\medskip

\noindent The removal of a requirements or used step mappings may be detected by a good static analysis of the generated source code. It will inform the developer of unused functions, that are suitable for removal.

\subsection{Implementation change}
Most implementation changes should be transparent to the tests, and only interfaces changes would really have an impact. As the test support tools from the case study system share the interfaces classes with the implementation, inconsistencies will be detected in a re-compilation of the test case sources. It is therefor important that the test support tools track implementation. In the case study system, this is done by putting an incremented version number on every release of the shared framework (see section \ref{ssec:openreception-framework}). The test runner will upgrade to the latest framework version, prior to building and running, hereby ensuring that the tests work with the latest interfaces.\medskip

\noindent If there is no sharing of interface code between tests and the developed system, it will very challenging and time consuming to keep the tests synchronized with the system. This is discussed further in section \ref{ssec:first-and-second-iteration-differences}.\medskip

\noindent Any change to actors or concepts seem unlikely, as they are created solely for the purpose of tests, and should only be changed if the requirements change.

\section{Benefits}
This section presents some of the benefits of using this procedure

\subsection{Requirement/test relationship}
%TODO Automatically generate tests.

\subsection{Simple use cases model}
The structuring of the use cases have been kept simple in order to make it flexible. Instead of just supporting one view of how, for instance, a use step would look like, mere textual representations are used for them. The mappings to the domain models of the system under test are provided by keyword identifications that refer to a domain actor or concept, via a definition. This makes it flexible in two ways: The first is that you do not need to define every actor and/or concept beforehand, but may simply write your use cases -- as use cases -- and add the definitions later. The second benefit is that the simple structure and textual representation makes it easy to export to other platforms, for example documentation.

\subsection{Mappings as source code}
The use cases mappings to system under test are defined as being source code that is to be written by developers (acting in the role of mappers). This gives the benefit of not having to train developers in a mapping language, but instead enable them to use a tool that should already know quite well -- general purpose programming. If the developers are the same as ones that develop the system under test, then there will be additional benefit of them already knowing how to interface with the system, in the context of writing up the test support tools.\medskip

\noindent Mappings as source code also give access to a wide range of static analysis source code tools, that provide useful information about the generated tests and written mappings, and detect errors in both. 

\subsection{Benefits}
A thing that was observed during the writeup of the tests using the second iteration of the test support tools was that writing general integration tests became a lot easier. The code written becomes very verbose in the way that you state the test from the point of view of an actor performing an action. Informally, what is actually possible, is writing a requirement as a test. Not as in the topic of this thesis, but as a hand-coded test that asserts that a feature is present. Having the tools (the test framework) makes it very intuitive and easy to write. The concrete code for testing the example outlined in section \ref{sec:parallelism} is shown in listing \ref{lst:test-code-single-call-allocation}. This code is not generated, but written by hand, using the test support tools.
\begin{lstlisting}[style=Dart, caption=Test code for single call allocation,label={lst:test-code-single-call-allocation}]
  static void pickupAllocatedCall(Receptionist receptionist, 
                                  Receptionist receptionist2, 
                                  Customer callee) {
    String receptionNumber = '12340002';
    Model.Call allocatedCall;
    
    log.info ('Customer ${callee.name} dials ${receptionNumber}');
    callee.dial (receptionNumber);
    log.info ('Receptionist ${receptionist.user.name} hunts call.');
    allocatedCall = receptionist.huntNextCall();
   
    expect (receptionist2.pickup(call), throwsA(Forbidden));
    log.info('Test done');
  }
\end{lstlisting}
Some may argue that this piece of code maps very nice to a requirement. While it may not be entirely possible to completely generated it from one, a requirement can most definitely be derived from the code. The example also captures an error condition, which may be challenging to describe from a use case in sufficient detail so that test generation is possible.\medskip

\noindent Another side benefit from having use cases written as tests -- whether they are generated or not -- gives new developers a good reference point on how the system is intended to work. By reading the class files representing actor and concepts, and how they interface with the system provides a good high-level overview of the general architecture, and how it should be used.

\section{Challenges}
This section contains some of the challenges faced during the analysis and implementation of the tool.
\subsection{Scalability}
\label{sec:scalability}
As mentioned in chapter \ref{ch:design}, the open issue on path coverage -- and especially scalability of it, remains. The number of paths generated quickly makes the size of the generated tests very large.\medskip

\noindent In order to overcome this additional heuristics, such as stricter design guidelines for use case writing may be enforced. If the use cases were limited to having a maximum number of extensions, or to be decomposed into several smaller use cases, it would be possible to keep the number of generated tests low.\medskip

\noindent Another angle on this would be to let the use case writer define which paths of the use case that should be selected for test generation. As an alternative to automatic generation, that had to generate a test for every path, it would be focused on the scenarios that are likely to happen when the system under test is in production.\medskip

\noindent As the approach presented in this thesis have not been applied to a system with more complicated use cases, it is difficult to say with certainty that it would not be possible to apply it on that system. It is however, very unlikely -- given the problem stated above -- that it would possible to use it without the heuristics also discussed.

\subsection{Side effects of use case steps}
While the view of having the system viewed as an environment -- or global state -- that is passed around and mutated, it does not solve the issue that sometimes use case steps will have side effects. And sometimes they are necessary for the next steps in the use case. Consider the following use case steps:

\begin{enumerate}
 \item Receptionist pickup inbound call
 \item Receptionist parks inbound call
 \item Receptionist locates an employee for dialing
 \item Receptionist dials employee
 \item Receptionist transfers the inbound call to the employee
\end{enumerate}

In this example, we are dealing with two calls which we can map to a ``Call'' class in our support test tools. The first call can be provided to test via a precondition. The other, however introduces two problems. The first is that the call has no explicit reference to a call being created. The keyword ``dials'' may be considered an active action, but as actions are no longer part of the meta model, this cannot be used. ``Employee'' is a role that is mapped to actor, so this leaves us with no information about that a ``Call'' object is being generated here. The other problem is derived from this. As we have no indication of the call being created, we will have means of tracking it either. So the last action -- ``Receptionist transfers the inbound call to the employee'' -- will not be able to be mapped without some way of sharing the created ``Call'' object between the two last steps.\medskip
A solution to this, which is also the way is should be solved in the current implementation, is to share these created objects via global variables -- which is a very error-prone and impure (software design-wise).

\subsection{Parallelism}
\label{sec:parallelism}
A case that is not able to be covered by this approach, is when a use case contains multiple actors that perform the same action simultaneously.\medskip

\noindent We have a use case, where two Receptionist actor battles for the same call. In order to test this, we may wish to describe the scenario like so -- assuming a call is has arrived and is ready for pickup:
\begin{enumerate}
 \item Receptionist 1 tries to pickup call
 \item Receptionist 2 tries to pickup call
 \item Call is assigned to either Receptionist
\end{enumerate}
%TODO add sequence diagram.
A postcondition may read ``The call is assigned to \emph{only} one receptionist'', in order to emphasize on the actual intended behavior of the system, but the description above should be sufficient.\medskip

\noindent Studying the use case a bit closer, what is actually implied is that 1. happens simultaneously with 2. In practice, it would mean that a test would have to emulate the simultaneous behavior by spawning multiple threads and collecting their return values once they have terminated. But there is another problem with the scenario above, which is that the test tools do not know when to parallelize. As of now, every step in a use case scenario is modeled as a synchronous action and will wait until the step has completed its execution before starting the next one.\medskip

\noindent A method for solving this, is to add the asynchronism in the mapped test code, but this is a very bad idea. This would lead to very unexpected behavior if requirements change in the specific block. This would lead to treads being spawned, expecting to perform a specific action that no longer existed in the requirements, perhaps deadlocking while waiting for an event to happen -- or change the state of the system that would lead an error later in the test.\medskip

\noindent A better way of solving it, is to add a keyword. For example, the word \textbf{simultaneously}. So the use case would then read;
\begin{enumerate}
 \item Receptionist 1 tries to pickup call
 \item Receptionist 2 tries to pickup call \textbf{simultaneously}
 \item Call is assigned to either Receptionist
\end{enumerate}
Making the keyword refer in 2. refer to the previous step, 1. This feature is neither implemented, nor conceptualized further, but included in the discussion as it is a actual problem that was encountered during the development of the 2nd iteration of the tests. There has been developed an \emph{ad-hoc} test that uses the spawn-threads-and-collect method introduced above so there exists a technical solution for the problem.

\section{Metrics}
This section presents the empiric evidence gathered with the purpose of determining the usefulness, additional workload factor of the approach in this thesis. We also try to determine the significance of the presence of a domain framework, integrated with the main code base. This is done by comparing failure levels for the first two iterations of the requirement translation tool (chapter \ref{chapter:first-iterations}).
\subsection{Domain framework significance}
\label{ssec:first-and-second-iteration-differences}
This section compares the average failure (and success) levels of the two iterations of the test support tools.
\begin{table}[!htbp]
\centering
\begin{tabular}{ | l | r | r |}
   \hline
   Project          & Avg. success rate & Avg. failure rate \\ \hline
   First iteration  & 53.4\%            & 46.6\%            \\
   Second iteration & 97.1\%            & 2.9\%             \\
   \hline
\end{tabular}
\caption{Success/failure rates for the two implementations}
\label{tab:jenkins-failure-rates}
\end{table}
The data set that is collected, are the graphs, and average success/failure rates from our continuous integration service (Jenkins CI). The most signification difference between the two implementations is the fact that one of them shares code with the code base of the developed system, while the other does not. This will, thus, serve as the evaluation point. The two datasets have different number of total tests, which is why the evaluation will be done on the basis of the \emph{relative} failure rate, instead of the absolute. Table \ref{tab:jenkins-failure-rates} document the average success and failure rate of tests. The numbers clearly state that the failure rate of the second implementation is significantly lower than the first. \medskip

\noindent In the first implementation of the test tools (see section \ref{sec:1st-iteration-test-specification}), there was no direct link to implementation code base. The tests were written in a different programming language than the main code base, and the implementation of clients that used the API interfaces were manual and duplicated.\medskip
\begin{figure}[!hbpt]
\centering
\includegraphics[width=1.0\textwidth]{\imgdir jenkins-build-trend-iteration-1}
\caption{Iteration 1 Jenkins build trend. Blue area is total tests, and red area is failures}
\label{fig:jenkins-build-trend-iteration-1}
\end{figure}

\noindent Iteration 1 build trend in figure \ref{fig:jenkins-build-trend-iteration-1}\footnote{Image quality is low, but as the Jenkins server that ran the test has been \emph{physically} garbage collected, it was the best that could be procured.} show an increasing relative number of errors compared to the second iteration shown in figure \ref{fig:jenkins-build-trend-iteration-2}.\medskip

\begin{figure}[!hbpt]
\centering
\includegraphics[width=1.0\textwidth]{\imgdir jenkins-build-trend-iteration-2}
\caption{Iteration 2 Jenkins build trend. Blue area is total tests, and red area is failures}
\label{fig:jenkins-build-trend-iteration-2}
\end{figure}

\noindent The second iteration treated tests as an intricate part of the main project, rather than an auxiliary part, and made use if the framework that was built for the application. This, ultimately led to a much lower failure rate, and the correction of errors much faster. First iteration has many longer-running errors, where second iteration typically fixes the error before new errors arise.\medskip

\noindent By this we conclude that the level of integration with the main code base matters. By sharing language and domain framework, the tests became an intricate part of the development, rather than an axillary part and the presence of domain framework is concluded to a critical part of this methodology.

\section{Workload overhead}
This section estimates and evaluates the workload associated with the methodology presented in this thesis.
\subsection{Test support tools}
To be able find a quantitative measure for how much extra work is involved in writing the test support tools, the number of code lines were enumerated, and support-only files were counted as overhead. Table \ref{tab:loc-metrics} show these metrics.\medskip

\begin{table}[!htbp]
\centering
\begin{tabular}{ | l | r | r | r |}
   \hline
   Project        & Total (kLoc) & Test support (kLoc) & Overhead \\ \hline
   Test framework & 8.9          & 1.1                 & 10\%     \\
   PhonIO library & 2            & 2                   & 100\%    \\
   \hline
   \hline
   Total          & 10.9         & 3.1                 & 28\%     \\
   \hline
\end{tabular}
\caption{Test framework metrics}
\label{tab:loc-metrics}
\end{table}

\noindent The rest of the test framework code are the (manually written) tests, but gives an indication of how much extra work is put into building these tests. In table \ref{tab:loc-metrics} we see that the overhead of test support tools in the test framework is 10\%, not counting the PhonIO library, which is built for the specific purpose of supporting our use case tests, but is actually not designed to be used only in our system. If we however do include it, our overhead increases to 28\%, which is a significant increase in code base size.\medskip

\noindent In order get a additional measure for the workload, the relative number of commits to the revision control system (RCS) was counted. Another motivation for include an additional measure, is that lines of code may be a bad measure\cite{fraser2013does}.\medskip

\begin{table}[!htbp]
\centering
\begin{tabular}{ | l | r | r | r |}
   \hline
   Project        & Total (commits) & Test support (commits) & Overhead \\ \hline
   Test framework & 355             & 53                     & 14.9\%   \\
   PhonIO         & 68              & 68                     & 100\%    \\
   \hline
   Total          & 10.9            & 3.1                    & 28.60\%  \\
   \hline
   
\end{tabular}
\caption{Number of commits (as of 2\textsuperscript{nd} of July, 2015)}
\label{tab:metrics-commit-count}
\end{table}

\noindent Table \ref{tab:metrics-commit-count} show the number of code commits performed over the last 5 months. It also show how many of these commits that involves the files that are support-only. Calculating the percentage overhead, we arrive at 14.9. However, taking into account the PhonIO library, we again arrive around 28\%. This is a lot higher than the overhead from a larger study of test-driven development methods\cite{george2003}. In this study, they approximate that roughly 16\% more time is spent on development when using test-driven development methods.\medskip

\noindent The metrics gathered in this section has given good indication of much extra effort is involved in writing test support tools. 28\% extra work is a substantial amount, and too much for a wide adoption as this will also mean an increase in man-hours spent, and a direct cost increase of the project.\medskip

The study of test-driven development methods mentioned above also showed that tdd approaches do produce higher quality software, so the extra time spend \emph{may} be worthwhile for some applications that are not cost-driven, but require a higher quality. There are, however, no metrics that support that this methodology produce higher quality software. The only thing that substantiates this claim, is the fact that feature (from a use case perspective) regressions are automatically detected when the

\subsection{Test mapping}
While the mapping functions are in a 1:1 relationship with the number of unique steps in the use cases, and if the interfaces and most of the domain models are provided to the tests via a domain framework, then the test mappings should be rather simple. The mapping functions in the use case tests of the case study system are, on average 4-6 lines of code -- including log statements.\medskip

\noindent We do however, also have more complicated tests, which are not part of the use cases. As the message-sending architecture is defined to be a work-queue where the dispatcher is decoupled from the message sending, we need additional support code to check if a message is received. If the postcondition for one our use cases had been; ``Message is received by contact'', then the function the provided this functionality would become quite complex. This is because our test support tools would need some way accessing an email mailbox. The single step of asserting that a message is actually received, would then result in hundreds of new lines of code.\medskip

\noindent The example above is an actual issue that we, in the development of the case study project, had to deal with. As we wanted to have our tests be as close to a deployed scenario as possible, we defined a test (not use case) that sent a message to number of recipients, and checked if the message was there. Other project may face similar challenges with, for instance physical hardware devices, that needs to be in certain state before or after the tests. Interfacing may not be available, and must then then be implemented for the sole purpose of testing.\medskip

\noindent In general; the use tests we have in the case study system are limited to functionalities provided via the domain framework, and no any external systems. This makes the case study system feasible for this methodology, with regards to the test mapping function overhead. The 4-6 lines pr. use case step is not a bit issue, as use case are short and simple. On the other hand, if another system had many external systems that needed to be interfaced with, then the test mappings becomes increasingly large and infeasible.

\section{Sources of error}
The case study system and test system has been developed by the same group of people, which have also played a large role in the use cases creation and refinement process. This is problematic when the general applicability of the approach presented in this thesis needs to be evaluated. Having the same people work on requirements, implementation and tests lead to a unified understanding of the system as a whole, and a very short path from requirement change, to implementation change. While this is generally a good thing, it does not scale to larger projects, and important domain-specific details may unknowingly go undocumented, as everyone has a common understanding of how things work overall. The problem with this, is also that requirements may have an ``artificial feel'', in the way that they reflect the actual workings of the system under development, rather than the intended workings.\medskip

\noindent For this evaluation, it means that our requirements may be better suited for acceptance-test-writing than others, trying the same approach. It can however be argued, that as any (good) requirement should be testable\cite{hull2010requirements}, then it shouldn't matter significantly how the project groups are structured.


%TODO maybe include this?
%\subsection{Recommended test framework guidelines}
%Use allocation pools
%Use interfaces a objects.


