\chapter{Discussion}
%TODO something about the moving taget (mainly how does injecting change look like, and how does it scale in this process?
%TODO is this integrateable in an exisiting development process?
This section discusses and evaluates the implementation from the thesis, establishing some quantifiable goals while discussing the general feasibility of the approach. Also raises some issues that needs to be solved before in order to improve the method, and identifies some of the side-benefits that followed the approach.


\section{General usefulness}
%TODO VERY IMPORTANT, as this gives the motivation for filling the thesis with crap about OpenReception project and architecture.
%is this always possible? Are we lucky to have found a good fit for this procedure? Which cases could be thought, not to support this method?

The process described in this thesis, fit quite nicely into the case study system. Is it a coincidence that the architecture described in chapter \ref{ch:background} fit very nicely. Another thing, is that the use cases -- in general -- are quite simple. This can also be seen in appendix \ref{appendix:use-cases}. The scope of the case study project has not been to provide a complex usage model, but rather the opposite. The target businesses for the product, focus on low average call-handling time and, thus, will try to keep the use cases simple -- in that effort. Another thing, that may have an effect on the specific fit, is the fact that the case study system uses a stateless architecture. This may make the use case steps easier to write, as the state is carried by the tests, rather than the services.\medskip

\noindent Other development projects, that have more complex use cases or interfaces, may experience larger complexity in writing mapping functions that realize their use case.

\section{Impact of changes}
This section tries to uncover the relative additional workload of this approach, from a requirement or implementation change perspective. Changes, thus, discussed from these two points of view.

\subsection{Requirement changes}
One of the most relevant question of this project is; how does requirement changes impact the generated tests? The answer to this, is defined by the characteristic of the change, which is either the modification of an existing requirement, or the addition of one.\medskip

\noindent The action of modifying an existing requirement, needs to follow a very specific procedure. First the use case needs to modified to accommodate the change, which could involve the rephrasing a single use case step, the addition or removal of an extension, or the (re)definition of a concept or actor. When this is done, the template will need to be changed to match the new/changed steps by either modifying existing mapping functions, or writing up new ones. The regeneration of tests is possible to do automatically by creating an association between use case and template, which is not present in the current implementation. This step is, thus manual. In essence; the workload of changing requirements is relative 1:1 relationship between changed/added steps and mapping functions. So one step changed, means one mapping function needs to be changed.\medskip

\noindent An addition of a new requirement, is defined by the same 1:1 workload, but has the additional task of creating the template needed to map to the implementation. If the new requirement defines a new actor -- or domain concept -- additional work must be calculated to build up the classes for these. A test tool overhead is presented in table \ref{tab:metrics-commit-count} that suggest that at least 15\% extra time is needed for writing up these classes.\medskip

\noindent The removal of a requirements or used step mappings may be detected by a good static analysis of the generated source code. It will inform the developer of unused functions, that are suitable for removal.

\subsection{Implementation change}
Most implementation changes should be transparent to the tests, and only interfaces changes would really have an impact. As the test support tools from the case study system share the interfaces classes with the implementation, inconsistencies will be detected in a re-compilation of the test case sources. It is therefor important that the test support tools track implementation. In the case study system, this is done by putting an incremented version number on every release of the shared framework (see section \ref{ssec:openreception-framework}). The test runner will upgrade to the latest framework version, prior to building and running, hereby ensuring that the tests work with the latest interfaces.\medskip

\noindent If there is no sharing of interface code between tests and the developed system, it will very challenging and time consuming to keep the tests synchronized with the system. This is discussed further in section \ref{ssec:first-and-second-iteration-differences}.\medskip

\noindent Any change to actors or concepts seem unlikely, as they are created solely for the purpose of tests, and should only be changed if the requirements change.

\section{Benefits}

\subsection{Simple use cases model}
The structuring of the use cases have been kept simple in order to make it flexible. Instead of just supporting one view of how, for instance, a use step would look like, mere textual representations are used for them. The mappings to the domain models of the system under test are provided by keyword identifications that refer to a domain actor or concept, via a definition. This makes it flexible in two ways: The first is that you do not need to define every actor and/or concept beforehand, but may simply write your use cases -- as use cases -- and add the definitions later. The second benefit is that the simple structure and textual representation makes it easy to export to other platforms, for example documentation.

\subsection{Source code mappings}
The use cases mappings to system under test are defined as being source code that is to be written by developers (acting in the role of mappers). This gives the benefit of not having to train developers in a mapping language, but instead enable them to use a tool that should already know quite well -- general purpose programming. If the developers are the same as ones that develop the system under test, then there will be additional benefit of them already knowing how to interface with the system, in the context of writing up the test support tools.\medskip

\noindent Mappings as source code also give access to a wide range of static analysis source code tools, that provide useful information about the generated tests and written mappings, and detect errors in both. 

\subsection{Side effects}
A thing that was observed during the writeup of the tests using the second iteration of the test support tools was that writing general integration tests became a lot easier. The code written becomes very verbose in the way that you state the test from the point of view of an actor performing an action. Informally, what is actually possible, is writing a requirement as a test. Not as in the topic of this thesis, but as a hand-coded test that asserts that a feature is present. Having the tools (the test framework) makes it very intuitive and easy to write. The concrete code for testing the example outlined in section \ref{sec:parallelism} is shown in listing \ref{lst:test-code-single-call-allocation}. This code is not generated, but written by hand, using the test support tools.
\begin{lstlisting}[style=Dart, caption=Test code for single call allocation,label={lst:test-code-single-call-allocation}]
  static void pickupAllocatedCall(Receptionist receptionist, 
                                  Receptionist receptionist2, 
                                  Customer callee) {
    String receptionNumber = '12340002';
    Model.Call allocatedCall;
    
    log.info ('Customer ${callee.name} dials ${receptionNumber}');
    callee.dial (receptionNumber);
    log.info ('Receptionist ${receptionist.user.name} hunts call.');
    allocatedCall = receptionist.huntNextCall();
   
    expect (receptionist2.pickup(call), throwsA(Forbidden));
    log.info('Test done');
  }
\end{lstlisting}
Some may argue that this piece of code maps very nice to a requirement. While it may not be entirely possible to completely generated it from one, a requirement can most definitely be derived from the code. The example also captures an error condition, which may be challenging to describe from a use case in sufficient detail so that test generation is possible.\medskip

\noindent Another side benefit from having use cases written as tests -- whether they are generated or not -- gives new developers a good reference point on how the system is intended to work. By reading the class files representing actor and concepts, and how they interface with the system provides a good high-level overview of the general architecture, and how it should be used.
\section{Challenges}
%TODO finalize this section
As already mentioned in chapter \ref{ch:design}, the open issue on path coverage -- and especially scalability of it, remains.

\subsection{Scalability}
\label{sec:scalability}
The number of paths generated quickly makes this procedure fail. Additional heuristics, such as strict design guidelines for use case writing -- espcially in regards to decomposition and relation to each other -- needs to be applied before it can be applied to larger systems.



\subsection{Parallelism}
\label{sec:parallelism}
A case that is not able to be covered by this approach, is when a use case contains multiple actors that perform the same action simultaneously.\medskip

\noindent We have a use case, where two Receptionist actor battles for the same call. In order to test this, we may wish to describe the scenario like so -- assuming a call is has arrived and is ready for pickup:
\begin{enumerate}
 \item Receptionist 1 tries to pickup call
 \item Receptionist 2 tries to pickup call
 \item Call is assigned to either Receptionist
\end{enumerate}
%TODO add sequence diagram.
A postcondition may read ``The call is assigned to \emph{only} one receptionist'', in order to emphasize on the actual intended behavior of the system, but the description above should be sufficient.\medskip

\noindent Studying the use case a bit closer, what is actually implied is that 1. happens simultaneously with 2. In practice, it would mean that a test would have to emulate the simultaneous behavior by spawning multiple threads and collecting their return values once they have terminated. But there is another problem with the scenario above, which is that the test tools do not know when to parallelize. As of now, every entry in a use case scenario is modeled as a synchronous action and will wait until the entry has completed its execution before starting the next one.\medskip

\noindent A method for solving this, is to add the asynchronism in the mapped test code, but this is a very bad idea. This would lead to very unexpected behavior if requirements change in the specific block. This would lead to treads being spawned, expecting to perform a specific action that no longer existed in the requirements, perhaps deadlocking while waiting for an event to happen -- or change the state of the system that would lead an error later in the test.\medskip

\noindent A better way of solving it, is to add a keyword. For example, the word \textbf{simultaneously}. So the use case would then read;
\begin{enumerate}
 \item Receptionist 1 tries to pickup call
 \item Receptionist 2 tries to pickup call \textbf{simultaneously}
 \item Call is assigned to either Receptionist
\end{enumerate}
Making the keyword refer in 2. refer to the previous entry, 1. This feature is neither implemented, nor conceptualized further, but included in the discussion as it is a actual problem that was encountered during the development of the 2nd iteration of the tests. There has been developed an \emph{ad-hoc} test that uses the spawn-threads-and-collect method introduced above so there exists a technical solution for the problem.



\section{Metrics}
This section presents the empiric evidence gathered with the purpose of determining the usefulness, additional workload factor of the approach in this thesis. We also try to determine the significance of the presence of a domain framework, integrated with the main code base. This is done by comparing failure levels for the first two iterations of the requirement translation tool (chapter \ref{chapter:first-iterations}).
\subsection{Domain framework significance}
\label{ssec:first-and-second-iteration-differences}
This section compares the average failure (and success) levels of the two iterations of the test support tools.
\begin{table}[!htbp]
\centering
\begin{tabular}{ | l | r | r |}
   \hline
   Project          & Avg. success rate & Avg. failure rate \\ \hline
   First iteration  & 53.4\%            & 46.6\%            \\
   Second iteration & 97.1\%            & 2.9\%             \\
   \hline
\end{tabular}
\caption{Success/failure rates for the two implementations}
\label{tab:jenkins-failure-rates}
\end{table}
The data set that is collected, are the graphs, and average success/failure rates from our continuous integration service (Jenkins CI). The most signification difference between the two implementations is the fact that one of them shares code with the code base of the developed system, while the other does not. This will, thus, serve as the evaluation point. The two datasets have different number of total tests, which is why the evaluation will be done on the basis of the \emph{relative} failure rate, instead of the absolute. Table \ref{tab:jenkins-failure-rates} document the average success and failure rate of tests. The numbers clearly state that the failure rate of the second implementation is significantly lower than the first. \medskip

\noindent In the first implementation of the test tools (see section \ref{sec:1st-iteration-test-specification}), there was no direct link to implementation code base. The tests were written in a different programming language than the main code base, and the implementation of clients that used the API interfaces were manual and duplicated.\medskip
\begin{figure}[!hbpt]
\centering
\includegraphics[width=1.0\textwidth]{\imgdir jenkins-build-trend-iteration-1}
\caption{Iteration 1 Jenkins build trend. Blue area is total tests, and red area is failures}
\label{fig:jenkins-build-trend-iteration-1}
\end{figure}

\noindent Iteration 1 build trend in figure \ref{fig:jenkins-build-trend-iteration-1}\footnote{Image quality is low, but as the Jenkins server that ran the test has been \emph{physically} garbage collected, it was the best that could be procured.} show an increasing relative number of errors compared to the second iteration shown in figure \ref{fig:jenkins-build-trend-iteration-2}.\medskip

\begin{figure}[!hbpt]
\centering
\includegraphics[width=1.0\textwidth]{\imgdir jenkins-build-trend-iteration-2}
\caption{Iteration 2 Jenkins build trend. Blue area is total tests, and red area is failures}
\label{fig:jenkins-build-trend-iteration-2}
\end{figure}

\noindent The second iteration treated tests as an intricate part of the main project, rather than an auxiliary part, and made use if the framework that was built for the application. This, ultimately led to a much lower failure rate, and the correction of errors much faster. First iteration has many longer-running errors, where second iteration typically fixes the error before new errors arise.\medskip

\noindent By this we conclude that the level of integration with the main code base matters. By sharing language and domain framework, the tests became an intricate part of the development, rather than an axillary part and the presence of domain framework is concluded to a critical part of this methodology.

\section{Workload overhead}
This section estimates and evaluates the workload associated with the methodology presented in this thesis.
\subsection{Test support tools}
To be able find a quantitative measure for how much extra work is involved in writing the test support tools, the number of code lines were enumerated, and support-only files were counted as overhead. Table \ref{tab:loc-metrics} show these metrics.\medskip

\begin{table}[!htbp]
\centering
\begin{tabular}{ | l | r | r | r |}
   \hline
   Project        & Total (kLoc) & Test support (kLoc) & Overhead \\ \hline
   Test framework & 8.9          & 1.1                 & 10\%     \\
   PhonIO library & 2            & 2                   & 100\%    \\
   \hline
   \hline
   Total          & 10.9         & 3.1                 & 28\%     \\
   \hline
\end{tabular}
\caption{Test framework metrics}
\label{tab:loc-metrics}
\end{table}

\noindent The rest of the test framework code are the (manually written) tests, but gives an indication of how much extra work is put into building these tests. In table \ref{tab:loc-metrics} we see that the overhead of test support tools in the test framework is 10\%, not counting the PhonIO library, which is built for the specific purpose of supporting our use case tests, but is actually not designed to be used only in our system. If we however do include it, our overhead increases to 28\%, which is a significant increase in code base size.\medskip

\noindent In order get a additional measure for the workload, the relative number of commits to the revision control system (RCS) was counted. Another motivation for include an additional measure, is that lines of code may be a bad measure\cite{fraser2013does}.\medskip

\begin{table}[!htbp]
\centering
\begin{tabular}{ | l | r | r | r |}
   \hline
   Project        & Total (commits) & Test support (commits) & Overhead \\ \hline
   Test framework & 355             & 53                     & 14.9\%   \\
   PhonIO         & 68              & 68                     & 100\%    \\
   \hline
   Total          & 10.9            & 3.1                    & 28.60\%  \\
   \hline
   
\end{tabular}
\caption{Number of commits (as of 2\textsuperscript{nd} of July, 2015)}
\label{tab:metrics-commit-count}
\end{table}

\noindent Table \ref{tab:metrics-commit-count} show the number of code commits performed over the last 5 months. It also show how many of these commits that involves the files that are support-only. Calculating the percentage overhead, we arrive at 14.9. However, taking into account the PhonIO library, we again arrive around 28\%. This is a lot higher than the overhead from a larger study of test-driven development methods\cite{george2003}. In this study, they approximate that roughly 16\% more time is spent on development when using test-driven development methods.\medskip

\noindent The metrics gathered in this section has given good indication of much extra effort is involved in writing test support tools. 28\% extra work is a substantial amount, and too much for a wide adoption as this will also mean an increase in man-hours spent, and a direct cost increase of the project. 

\subsection{Test mapping}
While the mapping functions are in a 1:1 relationship with the number of unique steps in the use cases, and if the interfaces and most of the domain models are provided to the tests via a domain framework, then the test mappings should be rather simple. The mapping functions in the use case tests of the case study system are, on average 4-6 lines of code -- including log statements.\medskip

\noindent We do however, also have more complicated tests, which are not part of the use cases. As the message-sending architecture is defined to be a work-queue where the dispatcher is decoupled from the message sending, we need additional support code to check if a message is received. If the postcondition for one our use cases had been; ``Message is received by contact'', then the function the provided this functionality would become quite complex. This is because our test support tools would need some way accessing an email mailbox. The single step of asserting that a message is actually received, would then result in hundreds of new lines of code.\medskip

\noindent The example above is an actual issue that we, in the development of the case study project, had to deal with. As we wanted to have our tests be as close to a deployed scenario as possible, we defined a test (not use case) that sent a message to number of recipients, and checked if the message was there. Other project may face similar challenges with, for instance physical hardware devices, that needs to be in certain state before or after the tests. Interfacing may not be available, and must then then be implemented for the sole purpose of testing.\medskip

\noindent In general; the use tests we have in the case study system are limited to functionalities provided via the domain framework, and no any external systems. This makes the case study system feasible for this methodology, with regards to the test mapping function overhead. The 4-6 lines pr. use case step is not a bit issue, as use case are short and simple. On the other hand, if another system had many external systems that needed to be interfaced with, then the test mappings becomes increasingly large and infeasible.

\section{Sources of error}
The case study system and test system has been developed by the same group of people, which have also played a large role in the use cases creation and refinement process. This is problematic when the general applicability of the approach presented in this thesis needs to be evaluated. Having the same people work on requirements, implementation and tests lead to a unified understanding of the system as a whole, and a very short path from requirement change, to implementation change. While this is generally a good thing, it does not scale to larger projects, and important domain-specific details may unknowingly go undocumented, as everyone has a common understanding of how things work overall. The problem with this, is also that requirements may have an ``artificial feel'', in the way that they reflect the actual workings of the system under development, rather than the intended workings.\medskip

\noindent For this evaluation, it means that our requirements may be better suited for acceptance-test-writing than others, trying the same approach. It can however be argued, that as any (good) requirement should be testable\cite{hull2010requirements}, then it shouldn't matter significantly how the project groups are structured.

\subsection{Evaluation as test-driven approach}
The TDD study also showed that tdd approaches do produce higher quality software, so the extra time spend \emph{may} be worthwhile.

In practice; the approach presented in this thesis is a variant of TDD, but injects some elements from MDE and increases the test coverage by automatically generating the tests from use case branches.

%Important bit.

%another note; it was a good experience for me, as a software engineer to be able communicate how I wanted the system to interact internally via tests. It is easier for code-oriented people to relate to code, rather than diagrams and designs.

\subsection{Is if feasible}
Basically; no. Can it be used? To some extent. The process is nice, but the mapping simply unfolds into a too large state space with a stucture so complex that is impossible for an end-user to specify it, and very hard for a trained professional to elaborate it. It may be feasible once model-based software engineering reaches a level where behavioral model can be applied to domain models, and thus ...
%As of now, an ad-hoc templating system is in place. A real templating system should be used in a real product.
The test coverage generation may be of some use, and also requirement changing
But, alas, it is too early to tell. More research is needed.


\subsection{Recommended test framework guidelines}
%Use allocation pools
%Use interfaces a objects.


%TODO The approach may also be used to support a top-down use case driven development approach, that is conceptually similar to test-driven development, in the way that it focuses -- after basic use cases are written -- on the basic functionality of the system from the point of view of a user. Writing the mappings to an implementation, in parallel with writing the implementation 
%TODO It motivates the question; what does this use case step mean in the system - from an interface point of view.
%TODO No defect rates have been measured.