\chapter{First iterations}
%Purpose is to provide anecdotal background for the theoretical descussion.
%First implementation focused on the generation, second focused on the domain.
This section describes the first and second iteration of the test generation tool. The first iteration was developed using code generation as an experiment that proved itself useful enough to refine. The second refinement was done as part of this thesis, and was linked directly to the shared domain model and interfaces of the system that was exposed in common framework (section \ref{ssec:openreception_framework}). The second iteration created a common ground for the requirements translation project, as many of the actors and concepts that we need to map to is provided to us by it (section \ref{ssec:supporting-test-tools}).\\
This section also covers the motivation and scoping for the tests and test tools. Later on, it explains how it was implemented and refined.
%TODO refer to the figure.
\begin{figure}[!htbp]
\centering
\begin{tikzpicture}

% horizontal axis
\draw[->] (0,0) -- (6,0) node[anchor=north,midway] {\small Automation};

% ranges
%\draw (4,3.5) node{{\scriptsize Test parameters}};

% vertical axis
\draw[->] (0,0) -- (0,4) node[anchor=south,rotate=90,midway] {\small Domain-awareness};
% nominal speed

\draw (2.5,1.5) node[circle,fill,inner sep=1pt, fill=dkgreen, label=above:Project] {}; %label
\draw (5,0.2) node[circle,fill,inner sep=1pt, fill=dkred,minimum size=0.3cm ,label=above:1st iteration] {}; %label
\draw (1.2,3.0) node[circle,fill,inner sep=1pt, fill=blue, label=above:2nd iteration] {}; %label

% Optimum
\draw (5,3) node[circle,fill,inner sep=1pt,label=above:Optimum] {}; %label

\end{tikzpicture}
\label{fig:project_parameter_plot_1st_iteration}
\caption{Project parameters with the 1st iteration highlighted}
\end{figure}
\section{Motivation and scope}
During the development of the system, little or no unit tests had prevailed. This  was primarily caused by the very high fluctuation in application architecture, design and implementation. None of the developers were familiar with the development of telephony applications, and lessons-learned frequently affected both design and implementation and quickly made whatever unit tests that was created, obsolescent. Far too much time was spent on refactoring code that, compiled fine, but no longer behaved as it should.\\
Is was mainly the high level of asynchronism of the system, and low level of control with the PBX that made feature regressions a daily pain. Unit tests were of little, or no use, as we were basically writing and testing components, without knowing if they belonged in the system, or not. For this problem; the solution of applying a test-driven development methods seemed very suitable, but we wanted to raise the abstraction level a bit further, and start directly from requirements and test the system as a whole.\\\\
Testing the system as a whole can be done in two ways; either by providing a complete test environment (sometimes called ``test harness'') that emulates the behavior of the interfaces that the component under test requires. In our case, the option of emulating the interface of an entire PBX wasn't really solution for our problem. Basically, if we knew the exact behavior of the PBX and IP-telephony domain, then we would just build the logic to handle it. The best solution we could think of, was to include the PBX as a required -- and included -- component for the component tests.
The second way you can test a system as whole, is to test a deployed instance of the system. But before anything could be tested, we needed to decide what to test.

\subsection{Test specification}
In order to derive a which cases should be tested, we -- in a figurative sense --  tore apart an activity diagram and created use cases from them. Then added the missing parts with the help of a representative from customer of the system. Once we had the use cases as a list of actions and alternatives, we had a manuscript that we needed to replay to assert functionality of the system.\\
In order to automate this, we estimated that every statement, or line, in the use case could be mapped to a block of manually written source. So, basically, if we had an action that was ``Receptionist dials extension of contact'', we would have a corresponding re-usable function that performed this action. We named this ``the use-case oriented scripting environment'', and used it to write up tests of the system that originated directly from the requirements.

\subsection{Mapping}
Every use case statement was analyzed, and concepts and actors were extracted from them, effectively giving us the minimal requirements for which objects (programming objects) that needed to participate in the test.\\
In the scripting environment, we then outlined the domain actors (such as caller and receptionists). External resources were also needed, and was provided by the main system via services interfaces. The final thing to outline and add to classes were the non-actor domain concepts, such as calls and messages.\\\\
The next step was then to assign every action, precondition and postcondition line a unique identification and map the identification to individual code chunks. We also wanted to generate documentation along with the tests, so the mapping from on entry was done to three separate chunks, each serving its own purpose.
\begin{description}
  \item[Visualization:] A graphical presentation of the use case in the form of a sequence diagram. In our specific case, we used a tool called seqdiag, and an example of the input is shown is shown in listing \ref{lst:seqdiag_code_example}. The main concept of this was to provide convenient overview of how, and when, the interaction between the actors happened.
  \item[Documentation:] In the OpenReception project, every bit of the documentation lives in a Wiki which is formatted using Markdown. This meant that we could link the source documents associated with use case to the use cases themselves. Generation of use case documentation in the Wiki was using these document fragments.
  \item[Testing] The final, and most significant part of the system, was the ability to generate use case tests from the actual use cases. This meant that we needed to describe every action in the use case as bit of code and then patch it together using the list of actions, pre- and postconditions provided in the use case descriptions. An example of an assembled is shown in listing \ref{lst:example_python_output}.
\end{description}
\begin{lstlisting}[language=Bash, caption=Example seqdiag input, label=lst:seqdiag_code_example]

Receptionist ->> System [label = "Dials contact"];
System       ->> Receptionist [label = "Call rejected"];
\end{lstlisting}

\begin{lstlisting}[language=Python, caption=Example generated Python code output, label=lst:example_python_output]
# \${WIKI_URL}
from forward_call import Test_Case
class Sequence_Diagram (Test_Case):
  def test_Run (self):
    self.Preconditions ()
    self.Receptionist_Dial_Contact()
    self.Call_Is_Denied()
    self.Postconditions()

\end{lstlisting}

\noindent
The observant reader will probably already have noticed that this code cannot stand by itself. For instance, the \texttt{self.Receptionist\_Dial\_Contact()} statement references a method found within its own class\footnote{For those unfamiliar with Python, \texttt{self} is a reference to the current object instance, similar to the \texttt{this} keyword in Java.}. In order to keep the code chunk complexity low, we decided to abstract a lot of the complexity into macro functions that were provided to the use case through a framework. Each use case was given it's own programming class that then provided these macro function via class members. Each class would then need to be self-contained and also provide setup/teardown functions, and setup pre and postconditions. The setup and teardown functions were defined as begin technical prerequisites, whereas the pre- and postcondition functions were for setting up the use case prerequisites and would, thus, map to the use case pre- and postconditions.\\\\
\begin{figure}[!h]
\centering
\includegraphics[width=0.20\textwidth]{\imgdir first_generation_class_structure}
\caption{Class diagram outlining the overall hierarchy of executable use cases}
\label{fig:first_generation_class_structure}
\end{figure}
Programing-wise, this was done by adding the needed macro methods required by the use cases to an abstract superclass that represented the overall use case. Each path, or variant, would then be an extension of this superclass. A simplified class diagram illustrating this is shown in figure \ref{fig:first_generation_class_structure}, where you can see that every variant (path of an extension) of a use case may reuse methods from its superclass. Each use case variant must also supply the abstract methods \texttt{run}, \texttt{Precondition} and \texttt{Postcondition} to fulfill the interface.\\
Missing methods, or problems in generated code were identified by static analysis using the pylint tool\footnote{http://www.pylint.org/}.
\newpage
\section{Second iteration}
We soon realized that doing individual mapping for every line was tedious, time consuming and error-prone. Especially due to the fact that a lot of implicit knowledge was needed to perform the manual mapping. An example of this would be to reference a person object in an a scenario action, assuming that it was previously declared in an earlier macro.\\\\
\begin{figure}[!htbp]
\centering
\begin{tikzpicture}

% horizontal axis
\draw[->] (0,0) -- (6,0) node[anchor=north,midway] {\small Automation};

% ranges
%\draw (4,3.5) node{{\scriptsize Test parameters}};

% vertical axis
\draw[->] (0,0) -- (0,4) node[anchor=south,rotate=90,midway] {\small Domain-awareness};
% nominal speed

\draw (2.5,1.5) node[circle,fill,inner sep=1pt, fill=dkgreen, label=above:Project] {}; %label
\draw (5,0.2) node[circle,fill,inner sep=1pt, fill=blue, label=above:1st iteration] {}; %label
\draw (1.2,3.0) node[circle,fill,inner sep=1pt, fill=dkred,minimum size=0.3cm ,  label=above:2nd iteration] {}; %label

% Optimum
\draw (5,3) node[circle,fill,inner sep=1pt,label=above:Optimum] {}; %label

\end{tikzpicture}
\label{fig:project_parameter_plot_2nd_iteration}
\caption{Project parameters with the 2nd iteration highlighted}
\end{figure}We thought that if we injected additional domain knowledge into the test framework, would reduce the number of errors associated in mapping the tests manually. Specifically, by moving our macro methods to the appropriate class -- in an object-oriented fashion -- would enable reuse. For example when a sentence states that a receptionist actor hangs up a call, we add a ``hangup'' method to the  ``Receptionist'' class that takes a ``Call'' object as an argument.

\todo[inline]{lead-over and refer to the figure. and add section explaining the content.}
The 2nd iteration is build during the course this thesis, and is thus considered a part of it.



\subsection{Sharing domain model knowledge}
During the evolution of the software, platform, language and architecture changes eventually landed us in a space where we had the opportunity to use the same programming language both on the sever and the client. We wanted to exploit this initially by sharing the model classes between the server and client, but it soon evolved into a larger framework, also covering interfaces and REST resource definitions. We could now, fairly easy, by re-using the domain classes and interfaces from the framework build tests that with a higher domain awareness than the previous iteration.\\
This section explains the processes of befitting from the models and interface of the framework for a third application: Namely testing. 

\begin{figure}
\centering
\includegraphics[width=0.80\textwidth]{\imgdir component_diagram_with_tests}
\caption{Component diagram -- extended with tests}
\label{fig:component_diagram_with_tests}
\end{figure}

\subsection{Framework content}
\label{ssec:openreception_framework}
The framework is a set of passive classes that is divided, into the following categories (and code libraries). It takes care of tedious low-level tasks such as serialization/deserialization object building from databases and Uri encoding. It is divided into the following different parts;
\begin{description}
  \item[Models:] Every model of the framework is supplied with methods that returns a DTO (Data Transfer Object) representation of the object
  \item[Storage:] Abstract interface descriptions. An example is a the interface that dictates which primitives a storage layer for messages must have. In our case, this is CRUD\todo{add glossary definition} operations, and and ``enqueue'' that puts a message into a delivery queue.
  \item[Service:] These classes use the framework model DTO deserialization methods to automatically build typed objects on the client side. This eliminates the hassle of having to deal with low-level encoding and decoding. It enables the clients to work directly on the domain objects via the supplied service interfaces.
  \item[Database:] Database access layer. Contains every SQL query functions. The functions casts from database queries to domain objects that may be serialized for transport to remote clients via -- for instance -- HTTP.
  \item[Resource:] Encoding library that turns objects into a REST-resource identifiers. Used to abstract away the Uri encodings on both the client and the server.
\end{description}
The framework served an important purpose in writing the code that modeled the actor behavior needed in the acceptance tests that we wrote.
\subsection{Supporting test tools}
%TODO Finish this section
\label{ssec:supporting-test-tools}
In order to perform fully automated end-to-end testing of the system, the framework was not quite enough. A black box test of a system -- in general -- requires external stimuli in order to produce a result.


We wanted to actually perform phone calls

\begin{description}
 \item[Resource pools:] Tests require resources that could be acquired from pools. Pools are resource containers with a fixed discrete amount of resources in them, or in some cases, containers with infinite amount of resources in them. Either they should hide behind macros, or provide components for other resources.
 \item[json-pjsua:] simple software SIP phone.
 \item[PhonIO:] Phone communication library
\end{description}
%TODO add some snibblets of framwork content.